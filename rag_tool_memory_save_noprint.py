import pandas as pd
from collections import defaultdict
from typing import List, Tuple, Dict
import json
import re
from difflib import SequenceMatcher
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.retrievers import BM25Retriever

def normalize(scores):
    min_s, max_s = min(scores), max(scores)
    if min_s == max_s:
        return [0.5] * len(scores)
    return [(s - min_s) / (max_s - min_s) for s in scores]


def ensemble_with_normalized_scores(docs1, docs2, weight1=0.6, weight2=0.4, top_k=10):
    def safe_uid(doc):
        app_no = doc.metadata.get("ì¶œì›ë²ˆí˜¸")
        chunk_id = doc.metadata.get("chunk_id")
        if app_no is None or chunk_id is None:
            return None
        return (str(app_no), str(chunk_id))  # ë¬¸ìì—´ë¡œ í†µì¼

    scores1 = {}
    scores2 = {}
    doc_map = {}

    for doc in docs1:
        uid = safe_uid(doc)
        if uid is not None:
            scores1[uid] = doc.metadata.get("score", 0)
            doc_map[uid] = doc

    for doc in docs2:
        uid = safe_uid(doc)
        if uid is not None:
            scores2[uid] = doc.metadata.get("score", 0)
            doc_map[uid] = doc

    uids = list(set(scores1.keys()).union(scores2.keys()))
    score_list1 = normalize([scores1.get(uid, 0) for uid in uids])
    score_list2 = normalize([scores2.get(uid, 0) for uid in uids])
    final_scores = [weight1 * s1 + weight2 * s2 for s1, s2 in zip(score_list1, score_list2)]

    # ì •ë ¬ ìˆ˜í–‰
    ranked = sorted(zip(final_scores, uids), reverse=True)
    return [doc_map[uid] for _, uid in ranked[:top_k]]


# ì‚¬í›„ í•„í„°ë§ í•¨ìˆ˜
def apply_post_filter(docs: List[Document], parsed_filter: Dict) -> List[Document]:
    def is_similar(a: str, b: str, threshold: float = 0.4) -> bool:
        return SequenceMatcher(None, a,
                               b).ratio() >= threshold  # ThresholdëŠ” í˜„ëŒ€ìë™ì°¨ë¥¼ í˜„ëŒ€ì°¨, í˜„ëŒ€ê¸°ì•„ì°¨, í˜„ëŒ€ ì™€ê°™ì€ ì—¬ëŸ¬ ê¸°ì—…ëª…ì— ì˜ ì ìš©ë˜ë„ë¡ ì¡°ì ˆí•˜ëŠ” í•¨ìˆ˜ì„

    filtered_docs = []
    for doc in docs:
        meta = doc.metadata
        keep = True

        if "ì¶œì›ì¸" in parsed_filter and isinstance(parsed_filter["ì¶œì›ì¸"], list) and parsed_filter["ì¶œì›ì¸"]:
            meta_applicant = meta.get("ì¶œì›ì¸", "")
            if meta_applicant == "íŠ¹ì • ê¸°ì—…":
                keep = True
            elif not any(is_similar(applicant, meta_applicant) for applicant in parsed_filter["ì¶œì›ì¸"]):
                keep = False

        if "ì¶œì›ì¼ì" in parsed_filter and isinstance(parsed_filter["ì¶œì›ì¼ì"], dict):
            doc_date = meta.get("ì¶œì›ì¼ì", None)
            if isinstance(doc_date, str):
                try:
                    doc_date = int(doc_date)
                except ValueError:
                    keep = False

            if isinstance(doc_date, int):
                for condition, target_value in parsed_filter["ì¶œì›ì¼ì"].items():
                    if condition == "$gte" and doc_date < target_value:
                        keep = False
                    if condition == "$lte" and doc_date > target_value:
                        keep = False

        if keep:
            filtered_docs.append(doc)

    return filtered_docs


def extract_json_from_text(text: str) -> str:
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        return match.group(0)
    raise ValueError("JSON í¬ë§·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def group_docs_by_application_number(docs: List[Document]) -> Dict[str, List[Document]]:
    grouped = defaultdict(list)
    for doc in docs:
        app_num = doc.metadata.get("ì¶œì›ë²ˆí˜¸", "UNKNOWN")
        grouped[app_num].append(doc)
    return grouped


def expand_documents_by_application_number(
        docs: List[Document],
        vectorstore: Chroma
) -> Tuple[List[Document], pd.DataFrame]:
    summarization_sections = {"ê¸°ìˆ ë°°ê²½", "ë°œëª…ì˜íš¨ê³¼"}
    structured_sections = {"ìš”ì•½", "ë…ë¦½ì²­êµ¬í•­", "ê¸°ìˆ ê³¼ì œ"}

    app_nums = list(set(doc.metadata.get("ì¶œì›ë²ˆí˜¸") for doc in docs if "ì¶œì›ë²ˆí˜¸" in doc.metadata))
    if not app_nums:
        return docs, pd.DataFrame(columns=["ì¶œì›ë²ˆí˜¸", "section", "ë‚´ìš©"])

    retriever = vectorstore.as_retriever(
        search_kwargs={"filter": {"ì¶œì›ë²ˆí˜¸": {"$in": app_nums}}, "k": 20}
    )
    expanded = retriever.get_relevant_documents("")

    # âœ… ì¤‘ë³µ ì œê±°
    seen = set()
    deduped = []
    for doc in docs + expanded:
        uid = (doc.metadata.get("ì¶œì›ë²ˆí˜¸", ""), doc.metadata.get("chunk_id", ""))
        if uid not in seen:
            seen.add(uid)
            deduped.append(doc)

    summary_docs = []
    structured_records = []

    for doc in deduped:
        app_num = doc.metadata.get("ì¶œì›ë²ˆí˜¸", "UNKNOWN")
        section = doc.metadata.get("section", "ê¸°íƒ€")
        content = doc.page_content.strip()

        if section in summarization_sections:
            summary_docs.append(doc)
        elif section in structured_sections:
            structured_records.append({
                "ì¶œì›ë²ˆí˜¸": app_num,
                "section": section,
                "ë‚´ìš©": content
            })

    # âœ… êµ¬ì¡°í™” ì •ë³´ â†’ DataFrame ë³€í™˜
    structured_info_df = pd.DataFrame(structured_records)
    return summary_docs, structured_info_df


def retrieve_structured_sections_by_appnums(app_nums: List[str], vectorstore: Chroma) -> pd.DataFrame:
    """
    ì„ íƒëœ ì¶œì›ë²ˆí˜¸ë³„ë¡œ 'ìš”ì•½', 'ê¸°ìˆ ê³¼ì œ', 'ë…ë¦½ì²­êµ¬í•­'ì„ ê°ê° ë³„ë„ë¡œ ë¦¬íŠ¸ë¦¬ë¸Œí•˜ì—¬ í†µí•©
    """
    structured_sections = {"ìš”ì•½", "ê¸°ìˆ ê³¼ì œ", "ë…ë¦½ì²­êµ¬í•­"}
    rows = []

    for app_num in app_nums:
        retriever = vectorstore.as_retriever(
            search_kwargs={"filter": {"ì¶œì›ë²ˆí˜¸": app_num}, "k": 30}
        )
        docs = retriever.get_relevant_documents("")

        for doc in docs:
            section = doc.metadata.get("section", "")
            if section in structured_sections:
                rows.append({
                    "ì¶œì›ë²ˆí˜¸": doc.metadata.get("ì¶œì›ë²ˆí˜¸", "N/A"),
                    "ì¶œì›ì¸": doc.metadata.get("ì¶œì›ì¸", "N/A"),
                    "section": section,
                    "ë‚´ìš©": doc.page_content.strip()
                })

    df = pd.DataFrame(rows)
    df["section"] = pd.Categorical(df["section"], categories=["ê¸°ìˆ ê³¼ì œ", "ìš”ì•½", "ë…ë¦½ì²­êµ¬í•­"], ordered=True)
    df.sort_values(["ì¶œì›ë²ˆí˜¸", "section"], inplace=True)
    return df


def generate_advanced_summary(selected_docs, user_query, llm):
    # âœ… ì¶œì›ë²ˆí˜¸ ê¸°ì¤€ ê·¸ë£¹í•‘ + ëª¨ë“  section ì €ì¥
    grouped = defaultdict(lambda: defaultdict(str))  # app_num â†’ section â†’ content

    for doc in selected_docs:
        app_num = doc.metadata.get("ì¶œì›ë²ˆí˜¸", "UNKNOWN")
        section = doc.metadata.get("section", "ê¸°íƒ€")
        content = doc.page_content.strip()
        # âœ… ì´ì œ ëª¨ë“  section í¬í•¨ (ì¤‘ë³µ ë°©ì§€)
        if section not in grouped[app_num]:
            grouped[app_num][section] = content  # ì²« ë“±ì¥ë§Œ ì €ì¥

    # âœ… Context êµ¬ì„± (ë…ë¦½ì²­êµ¬í•­ í¬í•¨)
    chunks = []
    for i, (app_num, sections) in enumerate(grouped.items(), start=1):
        example_doc = next(doc for doc in selected_docs if doc.metadata.get("ì¶œì›ë²ˆí˜¸") == app_num)
        title = example_doc.metadata.get("ë°œëª…ì˜ ëª…ì¹­", "N/A")
        applicant = example_doc.metadata.get("ì¶œì›ì¸", "N/A")

        doc_block = f"ì¶œì›ë²ˆí˜¸: {app_num}\nì¶œì›ì¸: {applicant}\në°œëª…ì˜ ëª…ì¹­: {title}"
        for section, content in sections.items():
            doc_block += f"\n[{section}]\n{content}"
        chunks.append(doc_block)

    context = "\n---------------------\n".join(chunks)

    # âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """
ë‹¹ì‹ ì€ ì „ê¸°ì°¨ ê¸°ìˆ  ë¶„ì•¼ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ë¶„ì„ê³¼ ë‹µë³€ì„ ì œê³µí•˜ëŠ” íŠ¹í—ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ContextëŠ” íŠ¹í—ˆë²ˆí˜¸ë³„ë¡œ ì§ˆë¬¸ê³¼ ê´€ê³„ ìˆëŠ” í•´ë‹¹ íŠ¹í—ˆì˜ ì •ë³´ê°€ ë‹´ê²¨ìˆìŠµë‹ˆë‹¤.
ì´ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ë˜, íŠ¹í—ˆê°€ ë“±ì¥í•˜ê²Œ ëœ ë°°ê²½ê³¼ ì–´ë–»ê²Œ ê¸°ì¡´ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì´ë¥¼ í†µí•œ ë°œëª…ì˜ íš¨ê³¼ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì‘ì„± ì§€ì¹¨]
- ì‚¬ìš©ì ì§ˆì˜ì™€ ê´€ë ¨ìˆëŠ” íŠ¹í—ˆë³„ ì„¤ëª…ì—ëŠ” ë°˜ë“œì‹œ "ì¶œì›ë²ˆí˜¸", "ì¶œì›ì¸", "ë°œëª…ì˜ ëª…ì¹­"ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.(ì„ì˜ë¡œ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.)
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì–´ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- íŠ¹í—ˆ ì„¤ëª…ì€ ìš”ì•½ì ì´ê³  ê°„ê²°í•˜ê²Œ, íë¦„ì€ ë¶€ë“œëŸ½ê²Œ ì—°ê²°í•˜ì„¸ìš”.
- ì´ë•Œ, ì„œë¡œ ë‹¤ë¥¸ íŠ¹í—ˆê°€ ë½‘í˜”ë‹¤ë©´, ê·¸ íŠ¹í—ˆì— ëŒ€í•œ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•´ë„ ë©ë‹ˆë‹¤.
"""
    user_prompt = """
ì•„ë˜ Contextì—ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì„ íƒëœ íŠ¹í—ˆ ì •ë³´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€ ì‘ì„± ì‹œ ë°˜ë“œì‹œ Contextì— í¬í•¨ëœ íŠ¹í—ˆë“¤ì˜ ëª¨ë“  'ì¶œì›ë²ˆí˜¸', 'ì¶œì›ì¸', 'ë°œëª…ì˜ ëª…ì¹­'ì„ ì •í™•íˆ ì¸ìš©í•˜ê³ , íŠ¹í—ˆë³„ ê¸°ìˆ  ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•˜ì„¸ìš”.
ê° íŠ¹í—ˆì˜ ë‚´ìš©ì€ Contextì—ì„œ '---------------------'ë¡œ ì¤„ë°”ê¿ˆë˜ì–´ êµ¬ë¶„ë˜ì–´ìˆìŠµë‹ˆë‹¤.

Context:
---------------------
{context}
---------------------

ì§ˆë¬¸:
{question}

==> Context ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ë…¼ë¦¬ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ì„ êµ¬ì„±í•˜ì„¸ìš”. ë‹¤ë§Œ, Context ì •ë³´ê°€ ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì—°ê´€ì´ ì—†ë‹¤ë©´ ë¬´ì‹œí•˜ì‹œê³ , ê´€ë ¨ëœ íŠ¹í—ˆë¥¼ ì°¾ê¸° ì–´ë µì§€ë§Œ ê°€ì§€ê³  ìˆëŠ” ì§€ì‹ ê¸°ë°˜ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""

    # âœ… ë©”ì‹œì§€ ì „ë‹¬
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt.format(context=context, question=user_query))
    ]

    # print("\nğŸ“¦ [Context Preview - ìš”ì•½ ì…ë ¥ ë‚´ìš©]")
    # print(context[:5000])  # ë„ˆë¬´ ê¸¸ ê²½ìš° ì• ë¶€ë¶„ë§Œ

    return llm.invoke(messages).content.strip()


def selector_filter_context(context_groups: Dict[str, List[Document]], query: str, llm, log_list=[]):
    selected_docs = []
    selected_app_nums = []
    for app_num, docs in context_groups.items():
        example = docs[0]
        title = example.metadata.get("ë°œëª…ì˜ ëª…ì¹­", "")
        applicant = example.metadata.get("ì¶œì›ì¸", "")
        summary = "\n".join([
            f"[{doc.metadata.get('section', '')}]\n{doc.page_content[:700]}"
            for doc in docs if doc.metadata.get("section") != "ë…ë¦½ì²­êµ¬í•­"
        ])

        prompt = f"""
        ì•„ë˜ëŠ” ì¶œì›ë²ˆí˜¸ {app_num}ì— í•´ë‹¹í•˜ëŠ” íŠ¹í—ˆì…ë‹ˆë‹¤:
        - ë°œëª…ì˜ ëª…ì¹­: {title}
        - ì¶œì›ì¸: {applicant}
        - ì£¼ìš” ë‚´ìš©:
        {summary}

        ì‚¬ìš©ì ì§ˆë¬¸:
        "{query}"

        ìœ„ì—ì„œ ë½‘íŒ íŠ¹í—ˆì˜ ë‚´ìš©ì´ ì‚¬ìš©ìì˜ ê¶ê¸ˆì¦ ë° ì§ˆë¬¸ì„ í•´ê²°í•˜ëŠ”ë° ê¸°ì—¬í•  ìˆ˜ ìˆìœ¼ë©´ 'Y', ê´€ë ¨ ì—†ìœ¼ë©´ 'N'ë§Œ ë‹µí•˜ì„¸ìš”.
        """
        response = llm.invoke([HumanMessage(content=prompt)]).content.strip()
        decision = response[0].upper()
        log_list.append({"ì¶œì›ë²ˆí˜¸": app_num, "ì¶œì›ì¸": applicant, "ë°œëª…ì˜ ëª…ì¹­": title, "ê²°ì •": decision})
        if decision == "Y":
            selected_docs.extend(docs)
            selected_app_nums.append(app_num)

    # âœ… ì„ íƒëœ ì¶œì›ë²ˆí˜¸ ì¶œë ¥
    # if selected_app_nums:
    #     print(f"\nâœ… ì„ íƒëœ ì¶œì›ë²ˆí˜¸ ({len(selected_app_nums)}ê°œ): {', '.join(selected_app_nums)}")
    # else:
    #     print("\nâŒ Selector íŒë‹¨ ê²°ê³¼: ì„ íƒëœ ì¶œì›ë²ˆí˜¸ ì—†ìŒ")

    return selected_docs


def run_filtered_rag(vectorstore, embedding_fn, user_query, llm, use_bm25=True):
    #  í•„í„°ë§ ì¡°ê±´ ì¶”ë¡ 
    filtering_prompt = PromptTemplate.from_template("""
ì‚¬ìš©ìì˜ ì§ˆì˜:
"{query}"

ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
- ì¶œì›ì¸: íŠ¹í—ˆë¥¼ ì¶œì›í•œ íŠ¹ì • ê¸°ì—… í˜¹ì€ íŠ¹ì • ì‚¬ëŒì˜ ì´ë¦„ì„ ì˜ë¯¸í•¨
ex) í˜„ëŒ€ìë™ì°¨, í˜„ëŒ€ëª¨ë¹„ìŠ¤, ì—˜ì§€ì´ë…¸í…, ì‚¼ì„±ì—ìŠ¤ë””ì•„ì´, ì—˜ì§€ì „ì, í…ŒìŠ¬ë¼, BYD ë“±
- ì¶œì›ì¼ì: íŠ¹í—ˆ ì¶œì› ì‹œì ì„ ì˜ë¯¸í•˜ë©°, ì—°ë„ë‹¨ìœ„ê°€ ë  ìˆ˜ ìˆìŒ                                                                        

ìœ„ í•­ëª© ì¤‘ ì–´ë–¤ í•­ëª©ì„ ì–´ë–¤ ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§í•´ì•¼ ê°€ì¥ ì ì ˆí•œ ê²€ìƒ‰ì´ ë ì§€ íŒë‹¨í•˜ë˜, í•„í„°ë§ ì¡°ê±´ì´ ì‚¬ìš©ìì˜ ì§ˆì˜ì— ì—†ëŠ”ë° êµ³ì´ í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
                                                    
ì‚¬ìš©ì ì§ˆì˜ì—ì„œ ì‚¬ìš©ìê°€ ìš”êµ¬í•˜ëŠ” ì •ë³´ëŠ” 
1)ì¶œì›ì¸ 
2)ì¶œì›ì¼ì 
ì´ë©°, 2ê°€ì§€ì˜ ì •ë³´ ì¤‘ í•˜ë‚˜ë§Œ ìš”êµ¬í•˜ê³  ìˆê±°ë‚˜, ë‹¤ ìš”êµ¬í•˜ê³  ìˆê±°ë‚˜, ëª¨ë‘ ìš”êµ¬í•˜ê³  ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ì´ì— ìœ ì˜í•˜ì—¬ filteringì„ ì‹œí–‰í•˜ì„¸ìš”.
ì´ë•Œ, ë°˜ë“œì‹œ **í•˜ë‚˜ì˜ JSON ê°ì²´**ë§Œ ì‘ì„±í•˜ê³  ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ íŠ¹ì • ì¡°ê±´ë“¤ì´ ì¶”ì¶œë˜ì§€ ì•Šì„ ê²½ìš°ëŠ” **ë¹ˆ JSON í˜•íƒœë¥¼ ì¶”ì¶œí•˜ì„¸ìš”**
ì•„ë˜ì˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶œë ¥ ì˜ˆì‹œ** (ë‹¤ë¥¸ ë¬¸ì¥ì€ ì“°ì§€ ë§ê³  ì´ í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”):
1. ì˜ˆë¥¼ ë“¤ì–´, "í˜„ëŒ€ìë™ì°¨ê°€ ì¶œì›í•˜ê³ , 2023ë…„ ì´í›„ì— ì¶œì›ëœ êµ¬ë™ëª¨í„° íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” ì‚¬ìš©ìê°€ ì¶œì›ì¸ê³¼ ì¶œì›ì¼ì ê¸°ì¤€ ì •ë³´ë¥¼ ëª¨ë‘ ìš”êµ¬í•˜ë¯€ë¡œ,
{{
    "ì¶œì›ì¸": [í˜„ëŒ€ìë™ì°¨],
    "ì¶œì›ì¼ì": {{"$gte": 20230101}}
}}
í˜•ì‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.                                                
2. ë‘ ë²ˆì§¸ ì˜ˆë¡œ,  "í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤ê°€ ì¶œì›í•œ 2022ë…„ ì´ì „ì˜ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” ì‚¬ìš©ìê°€ ì¶œì›ì¸ê³¼ ì¶œì›ì¼ì ê¸°ì¤€ ì •ë³´ë¥¼ ëª¨ë‘ ìš”êµ¬í•˜ë¯€ë¡œ,
{{
    "ì¶œì›ì¸": [í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤],
    "ì¶œì›ì¼ì": {{"$lte": 20220101}}
}}
í˜•ì‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

3. ì„¸ ë²ˆì§¸ ì˜ˆë¡œ, "í˜„ëŒ€ëª¨ë¹„ìŠ¤ì™€ ì—˜ì§€ì´ë…¸í…ì´ ì¶œì›í•œ 2020ë…„ê³¼ 2023ë…„ ì‚¬ì´ì˜ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” ì‚¬ìš©ìê°€ 2ê°€ì§€ ì¶œì›ì¸ê³¼ ì¶œì›ì¼ì ì •ë³´ë¥¼ ëª¨ë‘ ìš”êµ¬í•˜ë¯€ë¡œ,
{{
    "ì¶œì›ì¸": [í˜„ëŒ€ëª¨ë¹„ìŠ¤, ì—˜ì§€ì´ë…¸í…],
    "ì¶œì›ì¼ì":{{'$gte': 20220101, '$lte': 20240101}}
}}
í˜•ì‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.                                                
4. ë„¤ ë²ˆì§¸ ì˜ˆë¡œ, "2021ë…„ ì´ì „ì˜ ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” ì‚¬ìš©ìê°€ ì¶œì›ì¼ì ê¸°ì¤€ ì •ë³´ë§Œì„ ìš”êµ¬í•˜ë¯€ë¡œ,
{{
    "ì¶œì›ì¼ì":{{'$lte': 20240101}}                                           
}}
í˜•ì‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.                                                    
5. ë‹¤ì„¯ ë²ˆì§¸ ì˜ˆë¡œ, "2023ë…„ì— ë‚˜ì˜¨ í•˜ì´ë¸Œë¦¬ë“œ ì°¨ ì œë™ì¥ì¹˜ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” ì‚¬ìš©ìê°€ ì¶œì›ì¼ì ê¸°ì¤€ ì •ë³´ë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ,
{{
    "ì¶œì›ì¼ì":{{'$gte': 20230101 ,'$lte': 20240101}}                                           
}}
í˜•ì‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.                                                    
6. ì—¬ì„¯ë²ˆì§¸ ì˜ˆë¡œ, "ì—˜ì§€ì´ë…¸í…ì˜ ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” ì‚¬ìš©ìê°€ ì¶œì›ì¸ ê¸°ì¤€ ì •ë³´ë§Œì„ ìš”êµ¬í•˜ë¯€ë¡œ, 
{{
    "ì¶œì›ì¸":[ì—˜ì§€ì´ë…¸í…]                                          
}}
ë¡œ JSON í˜•ì‹ì„ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤.
                                                                                               
7. ë§ˆì§€ë§‰ ì˜ˆë¡œ, "ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ì¤˜" ë¼ëŠ” ì§ˆì˜ëŠ” **ì¶œì›ì¸**ê³¼ **ì¶œì›ì¼ì**ì— ëŒ€í•œ ì‚¬ìš©ìì˜ ì •ë³´ ìš”êµ¬ê°€ ëª¨ë‘ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
ë¹ˆ í˜•íƒœì˜ JSONì¸
{{}}ì„ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.
                              
ì°¸ê³ ë¡œ, ì‚¬ìš©ë˜ëŠ” í˜•ì‹ì—ì„œ ì¶œì›ì¼ìì˜ $gte ëŠ” íŠ¹ì • ì‹œì  ì´í›„, $lteëŠ” íŠ¹ì • ì‹œì  ì´ì „ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œê¸°ì…ë‹ˆë‹¤.
**íŠ¹ì • ê¸°ì—…** ì´ë¼ëŠ” ê²ƒìœ¼ë¡œ ì¶œì›ì¸ í•­ëª©ì„ í‘œê¸°í•˜ì§€ ë§ˆì„¸ìš”. ì—†ëŠ” ê²½ìš°ëŠ” JSONì„ ìƒì„±í•˜ì§€ ì•Šìœ¼ë©´ ë©ë‹ˆë‹¤.
í˜„ì¬ ì—°ë„ëŠ” 2025ë…„ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ 'ì‘ë…„', 'ì¬ì‘ë…„', 'Në…„ì „' ë“±ì— ëŒ€í•œ ìì—°ì–´ ì§ˆì˜ì— ì´ˆì ì„ ë§ì¶”ì–´ ìœ ë™ì ìœ¼ë¡œ ë²”ìœ„ë¥¼ ìƒì„±í•˜ì„¸ìš”.                                
"""
                                                    )
    filter_chain = LLMChain(llm=llm, prompt=filtering_prompt)
    try:
        parsed_filter = json.loads(extract_json_from_text(filter_chain.run(query=user_query)))
        filter_exists = True
    except:
        parsed_filter = {}
        filter_exists = False

    # print(f"ğŸ” ì¶”ë¡ ëœ í•„í„° ì¡°ê±´: {parsed_filter}")

    # 2. ì „ì²´ ë¬¸ì„œ ì¤€ë¹„
    all_data = vectorstore._collection.get(include=["documents", "metadatas"])
    all_docs = [
        Document(page_content=doc, metadata=meta)
        for doc, meta in zip(all_data["documents"], all_data["metadatas"])
    ]

    # 3. Ensemble Retriever with weight
    all_data = vectorstore._collection.get(include=["documents", "metadatas"])
    all_docs = [Document(page_content=doc, metadata=meta) for doc, meta in
                zip(all_data["documents"], all_data["metadatas"])]

    dense_retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 10, "fetch_k": 20})

    if use_bm25:
        all_data = vectorstore._collection.get(include=["documents", "metadatas"])
        all_docs = [Document(page_content=doc, metadata=meta) for doc, meta in
                    zip(all_data["documents"], all_data["metadatas"])]
        sparse_retriever = BM25Retriever.from_documents(all_docs)
        sparse_retriever.k = 10
        sparse_docs = sparse_retriever.get_relevant_documents(user_query)
    else:
        sparse_docs = []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬

    dense_docs = dense_retriever.get_relevant_documents(user_query)
    retrieved_docs = ensemble_with_normalized_scores(dense_docs, sparse_docs, top_k=10)
    sparse_retriever = BM25Retriever.from_documents(all_docs)
    sparse_retriever.k = 10

    dense_docs = dense_retriever.get_relevant_documents(user_query)
    sparse_docs = sparse_retriever.get_relevant_documents(user_query)
    retrieved_docs = ensemble_with_normalized_scores(dense_docs, sparse_docs, top_k=10)

    # print(f"\nğŸ” ì´ retrieval ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
    # for i, doc in enumerate(retrieved_docs[:10]):
    #     print(f"[ğŸ”¹ Retrieved {i+1}] {doc.metadata.get('ì¶œì›ë²ˆí˜¸', '')} | {doc.metadata.get('ì¶œì›ì¸', '')} | {doc.metadata.get('ë°œëª…ì˜ ëª…ì¹­', '')} | Section: {doc.metadata.get('section')}")

    if not retrieved_docs:
        # print("ğŸ“­ ê´€ë ¨ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŠ¹í—ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 4. ì‚¬í›„ í•„í„°ë§
    if filter_exists:
        filtered_docs = apply_post_filter(retrieved_docs, parsed_filter)
        if not filtered_docs:
            # print("ğŸ“­ í•„í„° í›„ ë¬¸ì„œ ì—†ìŒ")
            filtered_docs = retrieved_docs
            top_docs = retrieved_docs[:3]
    else:
        filtered_docs = retrieved_docs
        top_docs = filtered_docs[:3]

    # print(f"\nâœ… í•„í„°ë§ ì ìš© í›„ ë¬¸ì„œ ìˆ˜: {len(filtered_docs)}ê°œ")
    # for i, doc in enumerate(filtered_docs[:10]):
    #     print(f"[ğŸ”¸ Filtered {i+1}] {doc.metadata.get('ì¶œì›ë²ˆí˜¸', '')} | {doc.metadata.get('ì¶œì›ì¸', '')} | {doc.metadata.get('ë°œëª…ì˜ ëª…ì¹­', '')} | Section: {doc.metadata.get('section')}")

    top_docs = filtered_docs[:3]

    # 5. í™•ì¥ (ìš”ì•½ ëŒ€ìƒ, êµ¬ì¡°í™” ì •ë³´ ë¶„ë¦¬)
    summary_docs, structured_df = expand_documents_by_application_number(top_docs, vectorstore)
    summary_grouped = group_docs_by_application_number(summary_docs)

    # 6. Selector Loop (ìš”ì•½ ëŒ€ìƒ, êµ¬ì¡°í™” ì •ë³´ ë¶„ë¦¬)
    selector_log = []
    selected_docs = selector_filter_context(summary_grouped, user_query, llm, selector_log)

    if not selected_docs:
        # print("ğŸŒ€ Selector ì „ë¶€ ë¶€ì í•© â†’ Top 4~6 ì¬ì‹œë„")
        retry_docs = filtered_docs[3:6] if len(filtered_docs) >= 6 else retrieved_docs[3:6]
        retry_summary_docs, structured_df = expand_documents_by_application_number(retry_docs, vectorstore)
        retry_grouped = group_docs_by_application_number(retry_summary_docs)
        selected_docs = selector_filter_context(retry_grouped, user_query, llm, selector_log)

    #  ë‹¤ì‹œ ì‹¤íŒ¨ ì‹œ 3ì°¨ ì‹œë„
    if not selected_docs:
        # print("ğŸŒ€ Selector ì¬ì‹œë„ ì‹¤íŒ¨ â†’ Top 7~10 ì¬ì‹œë„")
        retry_docs = filtered_docs[6:10] if len(filtered_docs) >= 10 else retrieved_docs[6:10]
        retry_summary_docs, structured_df = expand_documents_by_application_number(retry_docs, vectorstore)
        retry_grouped = group_docs_by_application_number(retry_summary_docs)
        selected_docs = selector_filter_context(retry_grouped, user_query, llm, selector_log)

    if not selected_docs:
        # print("ğŸ“­ ìµœì¢…ì ìœ¼ë¡œ ì í•©í•œ íŠ¹í—ˆ ì—†ìŒ")
        # print("ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŠ¹í—ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        advanced_summary = "í•´ë‹¹ ê¸°ìˆ ê³¼ ê´€ë ¨í•œ íŠ¹í—ˆë¥¼ ì°¾ê¸° ì–´ë ¤ì›€"
        structured_info_df = ""
        return advanced_summary, structured_info_df

    # print(f"ğŸ“š Selector í†µê³¼ í›„ ë¬¸ì„œ ê°œìˆ˜: {len(selected_docs)}ê°œ")

    # ì„ íƒëœ ë¬¸ì„œ ê¸°ë°˜ Summary ì§„í–‰
    selected_app_nums = list(set(doc.metadata.get("ì¶œì›ë²ˆí˜¸", "") for doc in selected_docs))

    # 6. ê³ ê¸‰ ìš”ì•½ ìƒì„±
    advanced_summary = generate_advanced_summary(selected_docs, user_query, llm)
    summary_header = "\nğŸ“˜ íŠ¹í—ˆ ê²€ìƒ‰ ë° ìš”ì•½ ì •ë³´:\n"
    full_summary = summary_header + advanced_summary

    # Selector ì´í›„ ì¶œì›ë²ˆí˜¸ ì¶”ì¶œ
    selected_appnums = list({doc.metadata.get("ì¶œì›ë²ˆí˜¸") for doc in selected_docs})

    # Selector í†µê³¼ ëª»í•œ ì¶œì›ë²ˆí˜¸ ì¶”ì¶œ
    excluded_app_nums = list({
        doc.metadata.get("ì¶œì›ë²ˆí˜¸")
        for doc in retrieved_docs
        if doc.metadata.get("ì¶œì›ë²ˆí˜¸") not in selected_app_nums
    })

    # "ìš”ì•½" sectionë§Œ vectorstoreì—ì„œ ì¬ë¦¬íŠ¸ë¦¬ë¸Œ
    def retrieve_summary_sections_by_appnums(app_nums: List[str], vectorstore: Chroma, k: int = 20) -> List[Document]:
        """
        ì¶œì›ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ì¶œì›ë²ˆí˜¸ë³„ ë¬¸ì„œë¥¼ ë¦¬íŠ¸ë¦¬ë¸Œí•˜ê³ ,
        ê·¸ ì¤‘ sectionì´ 'ìš”ì•½'ì¸ ë¬¸ì„œë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜í•œë‹¤.
        """
        retrieved = []
        for app_num in app_nums:
            retriever = vectorstore.as_retriever(
                search_kwargs={
                    "filter": {"ì¶œì›ë²ˆí˜¸": app_num},
                    "k": k
                }
            )
            docs = retriever.get_relevant_documents("")
            retrieved.extend([doc for doc in docs if doc.metadata.get("section") == "ìš”ì•½"])
        return retrieved

    retrieved_excluded_summaries = retrieve_summary_sections_by_appnums(excluded_app_nums, vectorstore)

    # Selector í†µê³¼ ëª»í•œ ë¬¸ì„œ ìš”ì•½ ì œê³µìš© DF ìƒì„±

    excluded_summary_records = []
    for doc in retrieved_excluded_summaries:
        raw_summary = doc.page_content.strip()

        # ìš”ì•½, ìš”ì•½\n, ìš”ì•½: ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì œê±°
        cleaned_summary = re.sub(r"^ìš”ì•½[\s:\n]*", "", raw_summary)

        excluded_summary_records.append({
            "ì¶œì›ë²ˆí˜¸": doc.metadata.get("ì¶œì›ë²ˆí˜¸", "N/A"),
            "ì¶œì›ì¸": doc.metadata.get("ì¶œì›ì¸", "N/A"),
            "ë°œëª…ì˜ ëª…ì¹­": doc.metadata.get("ë°œëª…ì˜ ëª…ì¹­", "N/A"),
            "ìš”ì•½": cleaned_summary
        })

    excluded_summary_df = pd.DataFrame(excluded_summary_records)

    if not excluded_summary_df.empty:
        excluded_summary_text = "\n\nğŸ“„ Selectorì—ì„œ ì œì™¸ëœ íŠ¹í—ˆ ìš”ì•½ ì •ë³´:\n\n"
        excluded_summary_text += excluded_summary_df[["ì¶œì›ë²ˆí˜¸", "ë°œëª…ì˜ ëª…ì¹­", "ìš”ì•½"]].to_markdown(index=False)
    else:
        excluded_summary_text = ""

    return full_summary + "\n\n" + excluded_summary_text, excluded_summary_df
