from typing import Union, List, Tuple, Dict
from pydantic import BaseModel
import json
import pandas as pd
import re
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_community.chat_models import ChatOllama
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from rag_tool_memory_save_noprint import run_filtered_rag
from Final_Trend import KeywordAnalyzer
from Final_evaluator import Agent3
from writer_tool4_new import generate_technical_draft

llm = ChatOllama(model="qwen2.5:7b", temperature=0.0)

def safe_result_summary(result):
    if isinstance(result, pd.DataFrame):
        return result.to_markdown(index=False)
    elif isinstance(result, list):
        return "\n".join(str(x) for x in result[:5])
    elif isinstance(result, dict):
        return json.dumps(result, indent=2, ensure_ascii=False)
    else:
        return str(result)

def extract_json_from_text(text: str) -> str:
    json_pattern = r'\{[\s\S]*\}'
    match = re.search(json_pattern, text)
    if match:
        return match.group(0)
    raise ValueError("JSON \ud615\uc2dd\uc774 \uc544\ub2d9\ub2c8\ub2e4.")

embedding_model = OllamaEmbeddings(model="bge-m3")
vectorstore = Chroma(
    persist_directory="C:/Users/user/OneDrive/ë¬¸ì„œ/ì¢…ì„¤ì‹¤ìŠµ/streamlit/chroma_db_streamlit",
    embedding_function=embedding_model,
)

class PlanExecute(BaseModel):
    input: str
    tools: List[str] = []
    sub_queries: Dict[str, Union[str, List[str]]] = {}
    results: Dict[str, str] = {}
    response: Union[str, None] = None
    log: List[str] = []
    llm: ChatOllama = llm  # ì „ì—­ ê³µìœ  LLM

tool_selector_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì ì ˆí•œì§€ íŒë‹¨í•˜ì„¸ìš”.

[ë„êµ¬ ì„¤ëª…]
- patent_searcher: ì‚¬ìš©ìì˜ ê¸°ìˆ ì  ì§ˆë¬¸ì— ë§ëŠ” íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ê³  ê·¸ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
- patent_trend_analyzer: íŠ¹ì • í‚¤ì›Œë“œ, ì¶œì›ì¸ì„ ê¸°ì¤€ìœ¼ë¡œ íŠ¹í—ˆ ì¶œì› ê±´ìˆ˜ ë° íŠ¸ë Œë“œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
- patent_evaluator: ì£¼ì–´ì§„ ê¸°ìˆ  ì£¼ì œì— ëŒ€í•œ íŠ¹í—ˆë“¤ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ìì‚¬ê°€ ì¤‘ìš”í•˜ê²Œ ë³´ëŠ” ì§€í‘œë¥¼ í† ëŒ€ë¡œ ê²½ìŸì‚¬ì˜ ì¤‘ìš” ê´€ë ¨ íŠ¹í—ˆë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- tech_writer: ê¸°ìˆ  ê°œìš”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ  ì„¤ëª…ì„œ ì´ˆì•ˆì„ ì‘ì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
"{query}" 

âœ… ì„ íƒ ì§€ì¹¨:
- í•˜ë‚˜ì˜ ë„êµ¬ë§Œ ì„ íƒí•˜ëŠ” ê²ƒì´ ì›ì¹™ì…ë‹ˆë‹¤.
- ë‹¨, ì§ˆë¬¸ì´ ë‘ ê°œ ì´ìƒì˜ ë„êµ¬ê°€ í˜‘ì—…í•´ì•¼ë§Œ í•´ê²°ë˜ëŠ” ê²½ìš°, ë„êµ¬ë¥¼ ë³µìˆ˜ ì„ íƒí•˜ì„¸ìš”.
- patent_searcherì˜ ê²½ìš° ì§ˆë¬¸ì´ ë‘ ê°€ì§€ ì´ìƒì˜ ê³ ë„í™”ëœ íŠ¹í—ˆ ê²€ìƒ‰ì´ ìš”êµ¬ë˜ëŠ” ê²½ìš°, patent_searcherë¥¼ ë‘ ë²ˆ ë°˜ë³µ í˜¸ì¶œí•´ë„ ë©ë‹ˆë‹¤.
- patent_searcherê°€ 2ë²ˆ ì„ íƒëœ ê²½ìš°ëŠ” **ê·¸ë¦¬ê³ **, **ë˜ëŠ”** í˜¹ì€ ë§ˆì¹¨í‘œë‚˜ ì½¤ë§ˆë¡œ ì„œë¡œ ë‹¤ë¥¸ ì„±ê²©ì˜ ê¸°ìˆ ì— ëŒ€í•´ì„œ ì§ˆë¬¸í•˜ëŠ” ê²½ìš°ì— ë‘ ë²ˆ í˜¸ì¶œí•˜ë©´ë¼.

ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœì˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"tools": ["patent_searcher"]}}
ë˜ëŠ”
{{"tools": ["patent_searcher", "patent_evaluator"]}}
ë˜ëŠ”
{{"tools": ["patent_searcher", "patent_searcher"]}}
ë˜ëŠ”
{{"tools": ["patent_searcher", "patent_trend_analyzer"]}}
ë˜ëŠ”
{{"tools": ["patent_searcher", "patent_trend_analyzer","patent_evaluator"]}}
ë˜ëŠ”
{{"tools": ["tech_writer"]}}
""")

tool_selector_chain = tool_selector_prompt | llm

def tool_selector(state: PlanExecute):
    print("\nğŸ›  [STEP 1] ë„êµ¬ ì„ íƒ ì‹œì‘")
    response = tool_selector_chain.invoke({"query": state.input})
    raw = response.content.strip()
    print("ğŸ›  ë„êµ¬ ì„ íƒ ì‘ë‹µ ì›ë³¸:\n", raw)
    try:
        json_text = extract_json_from_text(raw)
        parsed = json.loads(json_text)
    except Exception as e:
        raise ValueError(f"[tool_selector] JSON íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ë³¸ ì‘ë‹µ:\n{raw}")
    print(f"ğŸ›  ì„ íƒëœ ë„êµ¬: {parsed['tools']}")
    return {"tools": parsed["tools"], "log": state.log + [f"ğŸ”§ ì„ íƒëœ ë„êµ¬: {parsed['tools']}"]}

sub_query_prompt = ChatPromptTemplate.from_template(r"""
ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì„ íƒëœ ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤.
ê° ë„êµ¬ì— ëŒ€í•´ í•˜ë‚˜ì˜ ì§ˆì˜ë¥¼ ìƒì„±í•˜ì„¸ìš”. 

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
"{query}"

ì„ íƒëœ ë„êµ¬ ëª©ë¡:
{tools}

âš ï¸ ì¶œë ¥ ì§€ì¹¨ (ì¤‘ìš”):
- ë°˜ë“œì‹œ ì¤‘ê´„í˜¸ {{}}ë¥¼ í¬í•¨í•œ ì™„ì „í•œ JSON ê°ì²´ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- ì˜ˆì‹œ:
```json
{{
  "sub_queries": {{
    "patent_searcher": "ì „ê¸°ì°¨ ë°°í„°ë¦¬ íš¨ìœ¨ ê°œì„  ê¸°ìˆ ",
    "patent_trend_analyzer": "2020ë…„ ì´í›„ ì „ê¸°ì°¨ ë°°í„°ë¦¬ ë™í–¥"
  }}
}}
```
""")

sub_query_chain = sub_query_prompt | llm

def tool_query_planner(state: PlanExecute):
    print("\nğŸ§© [STEP 2] Sub-query ìƒì„± ì‹œì‘")
    result = sub_query_chain.invoke({"query": state.input, "tools": ", ".join(state.tools)})
    print("ğŸ§© Sub-query ì‘ë‹µ ì›ë³¸:\n", result.content)
    try:
        json_text = extract_json_from_text(result.content)
        parsed = json.loads(json_text)
    except Exception as e:
        raise ValueError(f"âŒ Sub-query ìƒì„± ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨:\n{result.content}\n\nì—ëŸ¬: {e}")
    print("ğŸ§© Sub-query ë¶„ë¦¬ ê²°ê³¼:", parsed["sub_queries"])
    return {"sub_queries": parsed["sub_queries"], "log": state.log + [f"ğŸ§© Sub-query ë¶„ë¦¬ ê²°ê³¼: {parsed['sub_queries']}"]}

def execute_tools(state: PlanExecute):
    print("\nâš™ï¸ [STEP 3] ë„êµ¬ ì‹¤í–‰ ì‹œì‘")
    results = {}
    for tool_name, query in state.sub_queries.items():
        print(f"\nâ–¶ ì‹¤í–‰ ë„êµ¬: {tool_name}")
        print(f"â–¶ ì§ˆì˜ ë‚´ìš©: {query}")

        if tool_name == "patent_searcher":
            sub_queries = [q.strip() for q in query.split(",") if q.strip()]
            combined_summary = ""
            for i, sub_q in enumerate(sub_queries, 1):
                print(f"  ğŸ” [patent_searcher] ì„œë¸Œ ì¿¼ë¦¬ {i}: {sub_q}")
                summary, _ = run_filtered_rag(vectorstore, embedding_model, sub_q, llm=state.llm, use_bm25=True)
                combined_summary += f"[ì„œë¸Œ ì¿¼ë¦¬ {i}] {sub_q}\n{summary}\n\n"
            results[tool_name] = combined_summary.strip()

        elif tool_name == "patent_trend_analyzer":
            trend_agent = KeywordAnalyzer(csv_path="C:/Users/user/OneDrive/ë¬¸ì„œ/ì¢…ì„¤ì‹¤ìŠµ/streamlit/Codes/0527_cleaning_processing_ver1.csv", llm=state.llm)
            interpretation = trend_agent.run(query)
            results[tool_name] = safe_result_summary(interpretation)

        elif tool_name == "patent_evaluator":
            evaluator = Agent3(csv_path="C:/Users/user/OneDrive/ë¬¸ì„œ/ì¢…ì„¤ì‹¤ìŠµ/streamlit/Codes/0527_cleaning_processing_ver1.csv", llm=state.llm)
            interpretation = evaluator.handle(topic_query=query)
            results[tool_name] = interpretation

        elif tool_name == "tech_writer":
            result = generate_technical_draft.invoke({"user_input": query})
            content = result.content if hasattr(result, "content") else str(result)
            results[tool_name] = content

        else:
            results[tool_name] = f"[{tool_name}]ì— ëŒ€í•œ ì‘ë‹µ (Mock ì²˜ë¦¬ë¨)"

        print(f"âœ… {tool_name} ì‹¤í–‰ ì™„ë£Œ")

    print("âš™ï¸ ì „ì²´ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    return {"results": results, "log": state.log + [f"âš™ï¸ ì‹¤í–‰ ì™„ë£Œ: {list(results.keys())}"]}

def post_summary(state: PlanExecute):
    print("\nğŸ§  [STEP 4] ê²°ê³¼ ìš”ì•½ ì‹œì‘")
    merged = "\n\n".join(f"[{tool} ê²°ê³¼]\n{res}" for tool, res in state.results.items())

    if not merged.strip():
        print("âš ï¸ ê²°ê³¼ ì—†ìŒ: post_summary ë‹¨ê³„ì—ì„œ ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {"response": "â— ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", "log": state.log + ["âš ï¸ post_summary: ê²°ê³¼ ì—†ìŒ"]}

    summary_prompt = PromptTemplate.from_template("""

ì•„ë˜ëŠ” ê° ë„êµ¬ë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ê²°ê³¼ì…ë‹ˆë‹¤:

{merged}

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
ì´ë•Œ, ê° Toolì— ë”°ë¼ ì–´ë–¤ ì •ë³´ê°€ íŒŒì•…ë˜ì—ˆëŠ”ì§€, ì´ë¥¼ í†µí•´ ì–´ë–¤ ì¢…í•©ì  ì‹œì‚¬ì ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ë•Œ, **ë°˜ë“œì‹œ** í•œêµ­ì–´ë¡œë§Œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
""")
    summary_chain = summary_prompt | state.llm
    result = summary_chain.invoke({"merged": merged})

    print("ğŸ§  ìµœì¢… ì‘ë‹µ ìš”ì•½:\n", result.content.strip())
    return {"response": result.content.strip(), "log": state.log + ["ğŸ§  ìµœì¢… ì¢…í•© ìš”ì•½ ì™„ë£Œ"]}

graph = StateGraph(PlanExecute)
graph.add_node("tool_selector", tool_selector)
graph.add_node("tool_query_planner", tool_query_planner)
graph.add_node("execute", execute_tools)
graph.add_node("post_summary", post_summary)

graph.add_edge(START, "tool_selector")
graph.add_edge("tool_selector", "tool_query_planner")
graph.add_edge("tool_query_planner", "execute")
graph.add_edge("execute", "post_summary")
graph.add_edge("post_summary", END)

app = graph.compile(checkpointer=MemorySaver())
