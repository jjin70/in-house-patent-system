from typing import Union, List, Tuple, Dict
from pydantic import BaseModel
import json
import pandas as pd
import re
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_community.chat_models import ChatOllama
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from rag_tool_memory_save_noprint import run_filtered_rag
from Final_Trend import KeywordAnalyzer
from Final_evaluator import Agent3
from writer_tool4_new import generate_technical_draft

llm = ChatOllama(model="qwen2.5:7b", temperature=0.0)

def safe_result_summary(result):
    if isinstance(result, pd.DataFrame):
        return result.to_markdown(index=False)
    elif isinstance(result, list):
        return "\n".join(str(x) for x in result[:5])
    elif isinstance(result, dict):
        return json.dumps(result, indent=2, ensure_ascii=False)
    else:
        return str(result)

def extract_json_from_text(text: str) -> str:
    json_pattern = r'\{[\s\S]*\}'
    match = re.search(json_pattern, text)
    if match:
        return match.group(0)
    raise ValueError("JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

embedding_model = OllamaEmbeddings(model="bge-m3")
vectorstore = Chroma(
    persist_directory="/Users/heejinyang/python/streamlit/chroma_db_streamlit",
    embedding_function=embedding_model,
)

class PlanExecute(BaseModel):
    input: str
    tools: List[str] = []
    sub_queries: Dict[str, Union[str, List[str]]] = {}
    results: Dict[str, str] = {}
    response: Union[str, None] = None
    log: List[str] = []
    llm: ChatOllama = llm  # ì „ì—­ ê³µìœ  LLM

tool_selector_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì ì ˆí•œì§€ íŒë‹¨í•˜ì„¸ìš”.

[ë„êµ¬ ì„¤ëª…]
- patent_searcher: ì‚¬ìš©ìì˜ ê¸°ìˆ ì  ì§ˆë¬¸ì— ë§ëŠ” íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ê³  ê·¸ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
- patent_trend_analyzer: íŠ¹ì • í‚¤ì›Œë“œ, ì¶œì›ì¸ì„ ê¸°ì¤€ìœ¼ë¡œ íŠ¹í—ˆ ì¶œì› ê±´ìˆ˜ ë° íŠ¸ë Œë“œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
- patent_evaluator: ì£¼ì–´ì§„ ê¸°ìˆ  ì£¼ì œì— ëŒ€í•œ íŠ¹í—ˆë“¤ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ìì‚¬ê°€ ì¤‘ìš”í•˜ê²Œ ë³´ëŠ” ì§€í‘œë¥¼ í† ëŒ€ë¡œ ê²½ìŸì‚¬ì˜ ì¤‘ìš” ê´€ë ¨ íŠ¹í—ˆë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- tech_writer: ê¸°ìˆ  ê°œìš”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ  ì„¤ëª…ì„œ ì´ˆì•ˆì„ ì‘ì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
"{query}"

ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœì˜ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"tools": ["patent_searcher"]}}
ë˜ëŠ”
{{"tools": ["tech_writer"]}}
""")

tool_selector_chain = tool_selector_prompt | llm

def tool_selector(state: PlanExecute):
    response = tool_selector_chain.invoke({"query": state.input})
    raw = response.content.strip()
    print("ğŸ›  ë„êµ¬ ì„ íƒ ì‘ë‹µ ì›ë³¸:\n", raw)
    try:
        json_text = extract_json_from_text(raw)
        parsed = json.loads(json_text)
    except Exception as e:
        raise ValueError(f"[tool_selector] JSON íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ë³¸ ì‘ë‹µ:\n{raw}")
    print(f"ğŸ›  ì„ íƒëœ ë„êµ¬: {parsed['tools']}")
    return {"tools": parsed["tools"], "log": state.log + [f"ğŸ”§ ì„ íƒëœ ë„êµ¬: {parsed['tools']}"]}

sub_query_prompt = ChatPromptTemplate.from_template(r"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸:
"{query}"

ì„ íƒëœ ë„êµ¬ ëª©ë¡:
{tools}

âš ï¸ ì¶œë ¥ ì§€ì¹¨ (ì¤‘ìš”):
- ë°˜ë“œì‹œ ì¤‘ê´„í˜¸ {{}}ë¥¼ í¬í•¨í•œ ì™„ì „í•œ JSON ê°ì²´ í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- ì˜ˆì‹œ:
```json
{{
  "sub_queries": {{
    "patent_searcher": "ì „ê¸°ì°¨ ë°°í„°ë¦¬ íš¨ìœ¨ ê°œì„  ê¸°ìˆ ",
    "patent_trend_analyzer": "2020ë…„ ì´í›„ ì „ê¸°ì°¨ ë°°í„°ë¦¬ ë™í–¥"
  }}
}}
""")

sub_query_chain = sub_query_prompt | llm

def tool_query_planner(state: PlanExecute):
    result = sub_query_chain.invoke({"query": state.input, "tools": ", ".join(state.tools)})
    print("ğŸ§© Sub-query ì‘ë‹µ ì›ë³¸:\n", result.content)
    try:
        json_text = extract_json_from_text(result.content)
        parsed = json.loads(json_text)
    except Exception as e:
        raise ValueError(f"âŒ Sub-query ìƒì„± ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨:\n{result.content}\n\nì—ëŸ¬: {e}")
    print("ğŸ§© Sub-query ë¶„ë¦¬ ê²°ê³¼:", parsed["sub_queries"])
    return {"sub_queries": parsed["sub_queries"], "log": state.log + [f"ğŸ§© Sub-query ë¶„ë¦¬ ê²°ê³¼: {parsed['sub_queries']}"]}

def execute_tools(state: PlanExecute):
    results = {}
    for tool_name, query in state.sub_queries.items():
        if tool_name == "patent_searcher":
            sub_queries = [q.strip() for q in query.split(",") if q.strip()]
            combined_summary = ""
            for i, sub_q in enumerate(sub_queries, 1):
                summary, _ = run_filtered_rag(vectorstore, embedding_model, sub_q, llm=state.llm, use_bm25=True)
                combined_summary += f"[ì„œë¸Œ ì¿¼ë¦¬ {i}] {sub_q}\n{summary}\n\n"
            results[tool_name] = combined_summary.strip()
        elif tool_name == "patent_trend_analyzer":
            agent = KeywordAnalyzer(csv_path="/Users/heejinyang/python/streamlit/Codes/0527_cleaning_processing_ver1.csv", llm=state.llm)
            interpretation = agent.run(query)
            results[tool_name] = safe_result_summary(interpretation)
        elif tool_name == "patent_evaluator":
            agent = Agent3(csv_path="/Users/heejinyang/python/streamlit/Codes/0527_cleaning_processing_ver1.csv", llm=state.llm)
            interpretation = agent.handle(topic_query=query)
            results[tool_name] = interpretation
        else:
            results[tool_name] = f"[{tool_name}]ì— ëŒ€í•œ ì‘ë‹µ (Mock ì²˜ë¦¬ë¨)"
    return {"results": results, "log": state.log + [f"âš™ï¸ ì‹¤í–‰ ì™„ë£Œ: {list(results.keys())}"]}

def post_summary(state: PlanExecute):
    merged = "\n\n".join(f"[{tool} ê²°ê³¼]\n{res}" for tool, res in state.results.items())
    if not merged.strip():
        return {"response": "â— ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", "log": state.log + ["âš ï¸ post_summary: ê²°ê³¼ ì—†ìŒ"]}
    summary_prompt = PromptTemplate.from_template("""
ì•„ë˜ëŠ” ê° ë„êµ¬ë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ê²°ê³¼ì…ë‹ˆë‹¤:

{merged}

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
ì´ë•Œ, ê° Toolì— ë”°ë¼ ì–´ë–¤ ì •ë³´ê°€ íŒŒì•…ë˜ì—ˆëŠ”ì§€, ì´ë¥¼ í†µí•´ ì–´ë–¤ ì¢…í•©ì  ì‹œì‚¬ì ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ë•Œ, **ë°˜ë“œì‹œ** í•œêµ­ì–´ë¡œë§Œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
""")
    summary_chain = summary_prompt | state.llm
    result = summary_chain.invoke({"merged": merged})

    print("ğŸ§  ìµœì¢… ì‘ë‹µ ìš”ì•½:\n", result.content.strip())
    return {"response": result.content.strip(), "log": state.log + ["ğŸ§  ìµœì¢… ì¢…í•© ìš”ì•½ ì™„ë£Œ"]}

graph = StateGraph(PlanExecute)
graph.add_node("tool_selector", tool_selector)
graph.add_node("tool_query_planner", tool_query_planner)
graph.add_node("execute", execute_tools)
graph.add_node("post_summary", post_summary)

graph.add_edge(START, "tool_selector")
graph.add_edge("tool_selector", "tool_query_planner")
graph.add_edge("tool_query_planner", "execute")
graph.add_edge("execute", "post_summary")
graph.add_edge("post_summary", END)

app = graph.compile(checkpointer=MemorySaver())
